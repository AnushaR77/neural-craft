{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # Data Loading and Initial Exploration ","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv(\"train_complaints.csv\")\n\nprint(\"Data shape:\", df.shape)\nprint(\"\\nColumns:\", df.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(df.head())\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Preprocessing and Cleaning","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n\n    if not isinstance(text, str):\n        return \"\"\n    \n    text = str(text)\n    text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text, flags=re.MULTILINE)\n    text = re.sub(r'\\S+@\\S+', ' ', text)\n    text = re.sub(r'[^a-zA-Z\\s\\.\\,\\!\\?]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndf['cleaned_text'] = df['complaint_text'].apply(clean_text)\n\nprint(\"Text cleaning completed!\")\nprint(f\"Sample cleaned text:\\n{df['cleaned_text'][0][:200]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Severity Distribution Analysis","metadata":{}},{"cell_type":"code","source":"\nprint(\"Severity Distribution:\")\nseverity_counts = df['severity'].value_counts().sort_index()\nprint(severity_counts)\nprint(f\"\\nTotal samples: {len(df)}\")\nprint(f\"Number of severity levels: {df['severity'].nunique()}\")\n\nprint(\"\\nClass Balance (percentage):\")\nfor level in sorted(df['severity'].unique()):\n    percentage = (df['severity'] == level).sum() / len(df) * 100\n    print(f\"Severity {level}: {percentage:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train-Test Split with Stratification","metadata":{}},{"cell_type":"code","source":"\nX = df['cleaned_text']\ny = df['severity']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42,\n    stratify=y\n)\n\nprint(f\"Training set size: {len(X_train)}\")\nprint(f\"Test set size: {len(X_test)}\")\nprint(\"\\nTraining set class distribution:\")\nprint(y_train.value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TF-IDF Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"max_features=5000,\nngram_range=(1,2), Removes overly common words (appear in >85% of documents)","metadata":{}},{"cell_type":"code","source":"\ntfidf = TfidfVectorizer(\n    max_features=5000, \n    ngram_range=(1, 2),  \n    min_df=5,           \n    max_df=0.85        \n)\n\nprint(\"Creating TF-IDF features...\")\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\n\nprint(f\"TF-IDF shape - Train: {X_train_tfidf.shape}, Test: {X_test_tfidf.shape}\")\nprint(f\"Number of features: {len(tfidf.get_feature_names_out())}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baseline Model Training (Logistic Regression)","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, r2_score\n\nmodel = LogisticRegression(\n    class_weight='balanced',\n    max_iter=1000,\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"Training Logistic Regression model...\")\nmodel.fit(X_train_tfidf, y_train)\nprint(\"Training completed!\")\n\ny_pred = model.predict(X_test_tfidf)\ny_pred_proba = model.predict_proba(X_test_tfidf)\n\nprint(\"\\n Model Evaluation\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\nr2 = r2_score(y_test, y_pred)\nprint(f\"R² Score: {r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Advanced Feature Engineering ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\ndef create_additional_features(texts):\n    features = np.zeros((len(texts), 2))\n    for i, text in enumerate(texts):\n        features[i, 0] = len(text)  \n        features[i, 1] = len(text.split())  \n    return features\n\nX_train_extra = create_additional_features(X_train)\nX_test_extra = create_additional_features(X_test)\n\nfrom scipy.sparse import hstack\nX_train_combined = hstack([X_train_tfidf, X_train_extra])\nX_test_combined = hstack([X_test_tfidf, X_test_extra])\n\nprint(f\"Combined features shape: {X_train_combined.shape}\")\n\n\nrf_model = RandomForestClassifier(\n    n_estimators=150,\n    max_depth=20,\n    class_weight='balanced_subsample',  \n    random_state=42,\n    n_jobs=-1\n)\n\n\ngb_model = GradientBoostingClassifier(\n    n_estimators=150,\n    max_depth=10,\n    learning_rate=0.1,\n    random_state=42\n)\n\nlog_model = LogisticRegression(\n    class_weight='balanced',\n    max_iter=1000,\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"\\nTraining Random Forest model...\")\nrf_model.fit(X_train_combined, y_train)\n\nprint(\"Training Gradient Boosting model...\")\ngb_model.fit(X_train_combined, y_train)\n\nprint(\"Training Logistic Regression model...\")\nlog_model.fit(X_train_combined, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble Model Development","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, r2_score\nimport numpy as np\n\n\nprint(\"Making Predictions\")\n\n\nrf_pred = rf_model.predict(X_test_combined)\nprint(\"Random Forest Report:\")\nprint(classification_report(y_test, rf_pred, zero_division=0))\nrf_r2 = r2_score(y_test, rf_pred)\nprint(f\"Random Forest R²: {rf_r2:.4f}\")\n\n\ngb_pred = gb_model.predict(X_test_combined)\nprint(\"\\nGradient Boosting Report:\")\nprint(classification_report(y_test, gb_pred, zero_division=0))\ngb_r2 = r2_score(y_test, gb_pred)\nprint(f\"Gradient Boosting R²: {gb_r2:.4f}\")\n\n\nlog_pred = log_model.predict(X_test_combined)\nprint(\"\\nLogistic Regression Report:\")\nprint(classification_report(y_test, log_pred, zero_division=0))\nlog_r2 = r2_score(y_test, log_pred)\nprint(f\"Logistic Regression R²: {log_r2:.4f}\")\n\n\nprint(\"\\n=== Creating Weighted Ensemble ===\")\nensemble_pred = []\nfor i in range(len(y_test)):\n    predictions = [rf_pred[i], gb_pred[i], log_pred[i]]\n    ensemble_pred.append(np.bincount(predictions).argmax())\n\nprint(\"Ensemble Report:\")\nprint(classification_report(y_test, ensemble_pred, zero_division=0))\nensemble_r2 = r2_score(y_test, ensemble_pred)\nprint(f\"Ensemble R²: {ensemble_r2:.4f}\")\n\nmodels = {\n    'Random Forest': rf_r2,\n    'Gradient Boosting': gb_r2,\n    'Logistic Regression': log_r2,\n    'Ensemble': ensemble_r2\n}\nbest_model_name = max(models, key=models.get)\nprint(f\"\\nBest model for Severity: {best_model_name} (R²: {models[best_model_name]:.4f})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation and Comparison","metadata":{}},{"cell_type":"code","source":"print(\"Training Models for Primary Category\")\n\ny_primary = df['primary_category']\n\nX_train_primary, X_test_primary, y_train_primary, y_test_primary = train_test_split(\n    X, y_primary, test_size=0.2, random_state=42, stratify=y_primary\n)\n\ntfidf_primary = TfidfVectorizer(\n    max_features=4000,\n    ngram_range=(1, 2),\n    min_df=5,\n    max_df=0.9\n)\n\nX_train_primary_tfidf = tfidf_primary.fit_transform(X_train_primary)\nX_test_primary_tfidf = tfidf_primary.transform(X_test_primary)\n\nprimary_model = RandomForestClassifier(\n    n_estimators=150,\n    max_depth=20,\n    class_weight='balanced_subsample',\n    random_state=42,\n    n_jobs=-1\n)\n\nprimary_model.fit(X_train_primary_tfidf, y_train_primary)\nprimary_pred = primary_model.predict(X_test_primary_tfidf)\n\nprint(\"Primary Category Accuracy:\", accuracy_score(y_test_primary, primary_pred))\nprint(\"\\nPrimary Category Classification Report:\")\nprint(classification_report(y_test_primary, primary_pred, zero_division=0))\n\nprint(\"\\n=== Training Models for Secondary Category ===\")\ny_secondary = df['secondary_category']\n\nX_train_secondary, X_test_secondary, y_train_secondary, y_test_secondary = train_test_split(\n    X, y_secondary, test_size=0.2, random_state=42, stratify=y_secondary\n)\n\ntfidf_secondary = TfidfVectorizer(\n    max_features=5000,\n    ngram_range=(1, 2),\n    min_df=3,\n    max_df=0.85\n)\n\nX_train_secondary_tfidf = tfidf_secondary.fit_transform(X_train_secondary)\nX_test_secondary_tfidf = tfidf_secondary.transform(X_test_secondary)\n\nsecondary_model = GradientBoostingClassifier(\n    n_estimators=150,\n    max_depth=10,\n    learning_rate=0.1,\n    random_state=42\n)\n\nsecondary_model.fit(X_train_secondary_tfidf, y_train_secondary)\nsecondary_pred = secondary_model.predict(X_test_secondary_tfidf)\n\nprint(\"Secondary Category Accuracy:\", accuracy_score(y_test_secondary, secondary_pred))\nprint(\"\\nSecondary Category Classification Report (top 10 classes):\")\n\nunique_classes = np.unique(y_test_secondary)\nif len(unique_classes) > 10:\n  \n    top_classes = pd.Series(y_test_secondary).value_counts().head(10).index\n    mask = y_test_secondary.isin(top_classes)\n    print(classification_report(y_test_secondary[mask], secondary_pred[mask], zero_division=0))\nelse:\n    print(classification_report(y_test_secondary, secondary_pred, zero_division=0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_complaints.csv\")\nprint(\"Test data loaded, shape:\", test_df.shape)\n\ntest_df['cleaned_text'] = test_df['complaint_text'].apply(clean_text)\nprint(\"\\n Predicting Severity\")\n\nX_full_severity = df['cleaned_text']\ny_full_severity = df['severity']\n\ntfidf_full_severity = TfidfVectorizer(\n    max_features=5000,\n    ngram_range=(1, 2),\n    min_df=5,\n    max_df=0.85\n)\n\nX_full_tfidf = tfidf_full_severity.fit_transform(X_full_severity)\n\n\ndef create_features(text_series):\n    features = np.zeros((len(text_series), 2))\n    for i, text in enumerate(text_series):\n        features[i, 0] = len(text)\n        features[i, 1] = len(text.split())\n    return features\n\nX_full_extra = create_features(X_full_severity)\n\nfrom scipy.sparse import hstack\nX_full_combined = hstack([X_full_tfidf, X_full_extra])\n\nfinal_severity_model = GradientBoostingClassifier(\n    n_estimators=150,\n    max_depth=10,\n    learning_rate=0.1,\n    random_state=42\n)\nfinal_severity_model.fit(X_full_combined, y_full_severity)\n\nX_test_tfidf = tfidf_full_severity.transform(test_df['cleaned_text'])\nX_test_extra = create_features(test_df['cleaned_text'])\nX_test_combined = hstack([X_test_tfidf, X_test_extra])\n\ntest_severity = final_severity_model.predict(X_test_combined)\n\nprint(\" Predicting Primary Category\")\n\ny_full_primary = df['primary_category']\n\ntfidf_full_primary = TfidfVectorizer(\n    max_features=4000,\n    ngram_range=(1, 2),\n    min_df=5,\n    max_df=0.9\n)\nX_full_primary_tfidf = tfidf_full_primary.fit_transform(X_full_severity)\n\nfinal_primary_model = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=20,\n    class_weight='balanced_subsample',\n    random_state=42,\n    n_jobs=-1\n)\nfinal_primary_model.fit(X_full_primary_tfidf, y_full_primary)\n\nX_test_primary = tfidf_full_primary.transform(test_df['cleaned_text'])\ntest_primary = final_primary_model.predict(X_test_primary)\n\nprint(\" Predicting Secondary Category\")\n\ny_full_secondary = df['secondary_category']\n\ntfidf_full_secondary = TfidfVectorizer(\n    max_features=5000,\n    ngram_range=(1, 2),\n    min_df=3,\n    max_df=0.85\n)\nX_full_secondary_tfidf = tfidf_full_secondary.fit_transform(X_full_severity)\n\nfinal_secondary_model = GradientBoostingClassifier(\n    n_estimators=150,\n    max_depth=10,\n    learning_rate=0.1,\n    random_state=42\n)\nfinal_secondary_model.fit(X_full_secondary_tfidf, y_full_secondary)\n\nX_test_secondary = tfidf_full_secondary.transform(test_df['cleaned_text'])\ntest_secondary = final_secondary_model.predict(X_test_secondary)\n\nprint(\"All predictions completed!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'complaint_id': test_df['complaint_id'],\n    'primary_category': test_primary,\n    'secondary_category': test_secondary,\n    'severity': test_severity\n})\n\nsubmission.to_csv('final_submission_ensemble.csv', index=False)\n\nprint(\"Submission file created: final_submission_ensemble.csv\")\nprint(\"File preview:\")\nprint(submission.head())\nprint(f\"\\nFile shape: {submission.shape}\")\nprint(f\"Severity distribution in submission:\")\nprint(submission['severity'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}